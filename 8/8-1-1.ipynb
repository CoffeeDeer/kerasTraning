{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8-1-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O11-tlgEqkqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 글자 수준의 신경망 언어 모델 LSTM\n",
        "# character-level neural language model\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# original_distribution = 전체 합이 1인 1D numpy , softmax result\n",
        "def reweight_distribution(original_distribution, temperature = 0.5):\n",
        "  distribution = np.log(original_distribution) / temperature\n",
        "  distribution = np.exp(distribution)\n",
        "\n",
        "  return distribution / np.sum(distribution)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfNE4RvRuEsC",
        "colab_type": "code",
        "outputId": "d05d6b11-15b0-45f9-9549-f0659b3be829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "path = keras.utils.get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "text = open(path).read().lower()\n",
        "\n",
        "print('size', len(text))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "606208/600901 [==============================] - 0s 1us/step\n",
            "size 600893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9_GE2ycuWtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 60\n",
        "step = 3\n",
        "\n",
        "sentences = []\n",
        "\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, 10000, step):\n",
        "  sentences.append(text[i: i + maxlen])\n",
        "  next_chars.append(text[i + maxlen])\n",
        "\n",
        "  chars = sorted(list(set(text)))\n",
        "  char_indices = dict((char, chars.index(char)) for char in chars)\n",
        "\n",
        "  x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "  y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "\n",
        "\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "      x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I65IfY73vxF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(layers.Dense(len(chars), activation='softmax'))\n",
        "\n",
        "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCWrXSOZ03wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_s-3lrxF1BR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "c035b996-a850-4041-81ea-9712a45ab273"
      },
      "source": [
        "char_indices"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " \"'\": 4,\n",
              " '(': 5,\n",
              " ')': 6,\n",
              " ',': 7,\n",
              " '-': 8,\n",
              " '.': 9,\n",
              " '0': 10,\n",
              " '1': 11,\n",
              " '2': 12,\n",
              " '3': 13,\n",
              " '4': 14,\n",
              " '5': 15,\n",
              " '6': 16,\n",
              " '7': 17,\n",
              " '8': 18,\n",
              " '9': 19,\n",
              " ':': 20,\n",
              " ';': 21,\n",
              " '=': 22,\n",
              " '?': 23,\n",
              " '[': 24,\n",
              " ']': 25,\n",
              " '_': 26,\n",
              " 'a': 27,\n",
              " 'b': 28,\n",
              " 'c': 29,\n",
              " 'd': 30,\n",
              " 'e': 31,\n",
              " 'f': 32,\n",
              " 'g': 33,\n",
              " 'h': 34,\n",
              " 'i': 35,\n",
              " 'j': 36,\n",
              " 'k': 37,\n",
              " 'l': 38,\n",
              " 'm': 39,\n",
              " 'n': 40,\n",
              " 'o': 41,\n",
              " 'p': 42,\n",
              " 'q': 43,\n",
              " 'r': 44,\n",
              " 's': 45,\n",
              " 't': 46,\n",
              " 'u': 47,\n",
              " 'v': 48,\n",
              " 'w': 49,\n",
              " 'x': 50,\n",
              " 'y': 51,\n",
              " 'z': 52,\n",
              " 'ä': 53,\n",
              " 'æ': 54,\n",
              " 'é': 55,\n",
              " 'ë': 56}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGp5OOaU3w3o",
        "colab_type": "code",
        "outputId": "98dda199-0e48-4f94-9750-bc5b9160e450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "import sys\n",
        "\n",
        "random.seed(42)\n",
        "start_index = random.randint(0, 10000 -1)\n",
        "\n",
        "for epoch in range(1, 40):\n",
        "  print('epoch', epoch)\n",
        "  model.fit(x, y, batch_size=128, epochs=1)\n",
        "\n",
        "  seed_text = text[start_index: start_index + maxlen]\n",
        "  print('seed text:', seed_text)\n",
        "\n",
        "  for temperature in [0.01, 0.2, 0.5, 1.0, 1.2]:\n",
        "    if epoch > 10:\n",
        "      print('--------- temperature: ', temperature)\n",
        "    generated_text = seed_text\n",
        "    temp = generated_text\n",
        "    for i in range(400):\n",
        "      sampled = np.zeros((1, maxlen, len(chars)))\n",
        "      for t, char in enumerate(generated_text):\n",
        "        sampled[0, t, char_indices[char]] = 1.\n",
        "\n",
        "      preds = model.predict(sampled, verbose=0)[0]\n",
        "      next_index = sample(preds, temperature)\n",
        "      next_char = chars[next_index]\n",
        "\n",
        "      generated_text += next_char\n",
        "      generated_text = generated_text[1:]\n",
        "      temp += next_char\n",
        "    if epoch > 10:\n",
        "      print(temp)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "Epoch 1/1\n",
            "3334/3334 [==============================] - 6s 2ms/step - loss: 2.4065\n",
            "seed text: ibe themselves upon the heart of humanity with\n",
            "everlasting c\n",
            "epoch 2\n",
            "Epoch 1/1\n",
            "3334/3334 [==============================] - 6s 2ms/step - loss: 2.2957\n",
            "seed text: ibe themselves upon the heart of humanity with\n",
            "everlasting c\n",
            "epoch 3\n",
            "Epoch 1/1\n",
            "3334/3334 [==============================] - 5s 2ms/step - loss: 2.2098\n",
            "seed text: ibe themselves upon the heart of humanity with\n",
            "everlasting c\n",
            "epoch 4\n",
            "Epoch 1/1\n",
            "3334/3334 [==============================] - 5s 2ms/step - loss: 2.1026\n",
            "seed text: ibe themselves upon the heart of humanity with\n",
            "everlasting c\n",
            "epoch 5\n",
            "Epoch 1/1\n",
            "3334/3334 [==============================] - 7s 2ms/step - loss: 2.0072\n",
            "seed text: ibe themselves upon the heart of humanity with\n",
            "everlasting c\n",
            "epoch 6\n",
            "Epoch 1/1\n",
            "3334/3334 [==============================] - 5s 2ms/step - loss: 1.8901\n",
            "seed text: ibe themselves upon the heart of humanity with\n",
            "everlasting c\n",
            "epoch 7\n",
            "Epoch 1/1\n",
            "3334/3334 [==============================] - 6s 2ms/step - loss: 1.7683\n",
            "seed text: ibe themselves upon the heart of humanity with\n",
            "everlasting c\n",
            "epoch 8\n",
            "Epoch 1/1\n",
            "3334/3334 [==============================] - 5s 2ms/step - loss: 1.6454\n",
            "seed text: ibe themselves upon the heart of humanity with\n",
            "everlasting c\n",
            "epoch 9\n",
            "Epoch 1/1\n",
            "3334/3334 [==============================] - 6s 2ms/step - loss: 1.5118\n",
            "seed text: ibe themselves upon the heart of humanity with\n",
            "everlasting c\n",
            "epoch 10\n",
            "Epoch 1/1\n",
            "3334/3334 [==============================] - 5s 2ms/step - loss: 1.3484\n",
            "seed text: ibe themselves upon the heart of humanity with\n",
            "everlasting c\n",
            "epoch 11\n",
            "Epoch 1/1\n",
            "3334/3334 [==============================] - 6s 2ms/step - loss: 1.2114\n",
            "seed text: ibe themselves upon the heart of humanity with\n",
            "everlasting c\n",
            "--------- temperature:  0.01\n",
            "ibe themselves upon the heart of humanity with\n",
            "everlasting camiof it wemeaneself it wemeanstater of the the mome a hatemates athing ithist ut megemaamale of it has the selute at a hall the stions it wemeateself it wemeanstater of the the mome a hatemates athing ithist ut megemaamale of it has the selute at a hall the stions it wemeateself it wemeanstater of the the mome a hatemates of hat the selute at a hall the stions it wemeateself it wemeanstater of th\n",
            "--------- temperature:  0.2\n",
            "ibe themselves upon the heart of humanity with\n",
            "everlasting camionself the selute at at mast a fich athe not athing momalle of the solutereags of the remeamale of it mogmateself it remoteanstereagseland tommamestice the relust at deams atheis to the a momeaneated is hat the momle--aterions the the selust at defecessiag malose for the selute at a hall that the selustere sophis\n",
            "sole a sophicale it at meams athy cemomamaed ithis that it maght a filosiof the so\n",
            "--------- temperature:  0.5\n",
            "ibe themselves upon the heart of humanity with\n",
            "everlasting camemalofeisoustiaxs, a the selemale befiims-itaremonathereinist it befuites a hateabe befilated teremestater the selust it remosiof athings, a\n",
            "meamamateat of phalose wat the remallation of chat mame natereagamate came it we habe hand it it the mome ofioned ithuthere mast ofica thith a hiniteaterewhat ustiagstite hat the selueter of the stle the seluentersous\n",
            "of the seluntereamandareatiems\"irithe w\n",
            "--------- temperature:  1.0\n",
            "ibe themselves upon the heart of humanity with\n",
            "everlasting cthere have wiof? akendarereanesere, pephiof-itia llaabe spin\n",
            "talemomions hage hate, rowe witl of ithibe beficas atheis\n",
            "-ceuntaater a fondarig atrale? in itiof befice pander the romeameatencagd itsuch thich te have buens it tetion nve\n",
            "st ef ouiinatlesein theseigs-there momomo hat weale ofivarlace soursoutiondareamompanaee cemsouss,iy thacelunserneas\" irate.. bedureats leso hatstion nof meafsiatandy\n",
            "--------- temperature:  1.2\n",
            "ibe themselves upon the heart of humanity with\n",
            "everlasting cowiceusbaie ofichat matlleemof erop whe\n",
            "hing fomico\n",
            "be ritherely peistheste.\n",
            "wo inaik--ater af hate- usmhavel ewedacive ofir, as alett whsuexrcthe the gotiatismeof weame-itemsod in moseeneredhe\"-. cemaces), a df botiegalk it,rew\n",
            "w,a-lededezatiom--uf probs eit repherethetrexugs-oprated asoud of thwavelueattar sophal thveleioniof ithoug nhhare withermave suefis atseigsmete-ca itneit--tuith, a juliay\n",
            "epoch 12\n",
            "Epoch 1/1\n",
            "3334/3334 [==============================] - 5s 2ms/step - loss: 1.0661\n",
            "seed text: ibe themselves upon the heart of humanity with\n",
            "everlasting c\n",
            "--------- temperature:  0.01\n",
            "ibe themselves upon the heart of humanity with\n",
            "everlasting cer a lanind, the grouns, perhons, prodondad ther the selonty, procenty it a to donted torelon of the grounty iter ad ad carithe silf the spinits in for the selonty, perhonty, toreags the selonty, the groundy perhons the selonty, procenty it a to dact te to dicesy it a the selueny, withe to delenty it a to truth te haluedy it werions, prodondad ther the selonty, procenty it a to donted torelon of t\n",
            "--------- temperature:  0.2\n",
            "ibe themselves upon the heart of humanity with\n",
            "everlasting clamondanded beficher the selunty ithal tod the selueny, procented torelony, proce to to truth ter as of the selonty, procedned in beinitror of chall thed the groundy perhons the  alleny, the halueny, withe to dilused y the hal tod the selueny, withe to truth the filled it werhans it deam of the spicind dererond it a hate tor the selonty, perhonty, toreags the selonty, the groundy perhons the selon\n",
            "--------- temperature:  0.5\n",
            "ibe themselves upon the heart of humanity with\n",
            "everlasting ced a lon of chich the trists it a ter ad the salledy, ver the ficaly inand -ther the selueny, werhond treco to cally ither the salueny, procontin ts at dore for to in been and der to spions, the hal tod that the gariany of the ground it wer ass, werheds gond cerions, werhal -tion\n",
            "and a haconty, tor anly ither of beinis tred of linengy, wereags the tond tued it as ats, arle for the selon for ersoug\n",
            "--------- temperature:  1.0\n",
            "ibe themselves upon the heart of humanity with\n",
            "everlasting cerhers\"s in to abll theself thredhed suin bein so hrcontion and, ism, ray tings,is to frion -veryigtsuthar mon ancle seansy weljtrady of the sping, and t wht a fortegs, toy that the sopdito hais it wareary befiched of corionty, velontod of calustye to eamondied--werhad tions,, hare of trem. bufindasty,, trag ton the teving,\n",
            "treceastved\n",
            "of the sepinnt, it is, wanden.\n",
            "ngeaded pricaldy  for adry, ero\n",
            "--------- temperature:  1.2\n",
            "ibe themselves upon the heart of humanity with\n",
            "everlasting calion, val tatitseyned er the tertave kve splen\n",
            "ved it\n",
            "allnor on onclinithtrt,\n",
            "arly protrested a adion en and the sas\n",
            "andyy orrrhod of tre sinetyejy-hat who trist than aginted orhran therf buinst serhednbly zedrctons, wer the faluef procy, and the gond--a aniadal d,, in warder of there wicit\n",
            "ar to uphainty, to calomy it.n verions, whomes ly adl eeced lufer the splacady, prezalosy g.fwis\n",
            "ins idalip\n",
            "epoch 13\n",
            "Epoch 1/1\n",
            "3334/3334 [==============================] - 6s 2ms/step - loss: 0.9390\n",
            "seed text: ibe themselves upon the heart of humanity with\n",
            "everlasting c\n",
            "--------- temperature:  0.01\n",
            "ibe themselves upon the heart of humanity with\n",
            "everlasting cand commaniteseress\" hind athings, and came a dor the sormanistare to the pristeall that the sermonsle that the sermonsle that the sermonsle that the sermonsle that the sermonsle that the sermonsle that the sermonsle that the sermons, and the soplrers wor the willes of the the suproplenses, the the sugmontale st and a dowe of the the suprops, and the sthe tremand and in and a mond athings, and cam\n",
            "--------- temperature:  0.2\n",
            "ibe themselves upon the heart of humanity with\n",
            "everlasting cand commanites, ind in ind and it were for the saluty it a that the selonto it all dor the sormanitealued--there sperstonand and camone not the wind amond ctrust a domeangalang,, the th, who trethese for the soplinits and the sthe tremand and inmand at a the solluth whr the pristeand the to the pristeall that the sulontion.\n",
            "and and a mondagmand, the the sustouss and a dormatistale their ton and a \n",
            "--------- temperature:  0.5\n",
            "ibe themselves upon the heart of humanity with\n",
            "everlasting coucicalvary itoust a hadionalle \"sthat the grout the sermonstions, were kond torm wonl to the prosteencalang,, the tremand and inmand ath were for the pristeand inmanderof the the grouste, the throug tor the mainted tor the prelostoperseste. and the wonl trome pricalond, the to duganaonale sophiristale their \"doper, wither the grouns, ar the to the pricaanver of the sthe groust at the selonto-itha\n",
            "--------- temperature:  1.0\n",
            "ibe themselves upon the heart of humanity with\n",
            "everlasting cand have tion. the tr, wro have wilate? wand of rithas pephandans, aly thae sopprhirstale sechat the dommalssophrist, the mactemugandagsref itandingastinte-tand tef conclinats-perhistalenabker4o, sfillect pamis, and a doluct of hat who trithrerte,\n",
            "ant, whe to maintary a. that the galteataren, redope, a hace spistanntareryoge, sothro\n",
            "hount restable pfonste, arf himateralinouly, rersthere sof the tr\n",
            "--------- temperature:  1.2\n",
            "ibe themselves upon the heart of humanity with\n",
            "everlasting cand \"be who in, and cammable thiak thair thar sionand tor the wigle to tugengeariens),\n",
            "rotha\n",
            "\n",
            "valluef-ion-it, pirhass\"\n",
            "reinpouruce is aos dermave yeans of usphans onvy is to sitf tem, and cammamomonater, and tre halt beinatew,pt at llemexicalil, som, or. to therr ousssuand camiantarechers had the tr.\n",
            "but, re\n",
            "eod in, toy nobl to truth prowewoustres-of tras the ef-thas  of tre belkob on, the to\n",
            "more\n",
            "epoch 14\n",
            "Epoch 1/1\n",
            "3334/3334 [==============================] - 6s 2ms/step - loss: 0.8059\n",
            "seed text: ibe themselves upon the heart of humanity with\n",
            "everlasting c\n",
            "--------- temperature:  0.01\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-13def9ed7e94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m       \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If0gzdls5SeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}