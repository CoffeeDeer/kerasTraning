{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8-2-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Jg6A-6VNg5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "21f785b7-0d9e-4a13-c8bc-2d6ac9ac72ed"
      },
      "source": [
        "from keras.applications import inception_v3\n",
        "from keras import backend as K\n",
        "\n",
        "# do not model traning\n",
        "K.set_learning_phase(0)\n",
        "\n",
        "model = inception_v3.InceptionV3(weights='imagenet', include_top=False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQI5HIRKVy8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_contributions = {\n",
        "    'mixed2': 0.2,\n",
        "    'mixed3': 3.,\n",
        "    'mixed4': 2.,\n",
        "    'mixed5': 1.5,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFOxhUrvW9FL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "\n",
        "loss = K.variable(0.)\n",
        "for layer_name in layer_contributions:\n",
        "  coeff = layer_contributions[layer_name]\n",
        "\n",
        "  # get layer output\n",
        "  activation = layer_dict[layer_name].output \n",
        "\n",
        "  scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n",
        "\n",
        "  # 층 득성의 L2 노름 제곱을 손실에 추가 \n",
        "  loss = loss + (coeff * K.sum(K.square(activation[:, 2:-2, 2:-2, :])) / scaling)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XxjdZYWZnwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 생성된 딥드림 이미지를 저장 \n",
        "dream = model.input\n",
        "\n",
        "# 손실에 대한 딥드림 이미지의 그래디언트를 계산 - 왜? \n",
        "grads = K.gradients(loss, dream)[0]\n",
        "\n",
        "# 그래디언트를 정규화 - 이게 중요????\n",
        "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n",
        "\n",
        "# 주어진 입력 이미지에서 손실과 그래디언트 값을 계산할 케라스 funtion 객체를 만듬 \n",
        "outputs = [loss, grads]\n",
        "fetch_loss_and_grads = K.function([dream], outputs)\n",
        "\n",
        "def eval_loss_and_grads(x):\n",
        "  outs = fetch_loss_and_grads([x])\n",
        "  loss_value = outs[0]\n",
        "  grad_value = outs[1]\n",
        "\n",
        "  return loss_value, grad_value\n",
        "\n",
        "# 경사 상승법을 여러번 반복? \n",
        "def gradient_axcent(x, iterations, step, max_loss=None):\n",
        "  for i in range(iterations):\n",
        "    loss_value, grad_value = eval_loss_and_grads(x)\n",
        "    if max_loss is not None and loss_value > max_loss:\n",
        "      break\n",
        "    print('...', i,' 번째 손실 :', loss_value)\n",
        "    x += step * grad_values\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO7Ednv6jemI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "step = 0.01\n",
        "num_octave = 3\n",
        "octave_scale = 1.4\n",
        "iterations = 20\n",
        "\n",
        "max_loss = 10.\n",
        "\n",
        "base_image_path = '/content/drive/My Drive/Colab Notebooks/creative_commons_elephant.jpg'\n",
        "\n",
        "img = preprocess_image(base_image_path)\n",
        "\n",
        "original_shape = img.shape[1:3]\n",
        "successive_shapes = [original_shape]\n",
        "\n",
        "for i in range(1, num_octave):\n",
        "  shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n",
        "  successive_shapes.append(shape)\n",
        "\n",
        "successive_shapes = successive_shapes[::-1]\n",
        "\n",
        "original_img = np.copy(img)\n",
        "shrunk_origianl_img = resize_img(img, successive_shapes[0])\n",
        "\n",
        "for shape in successive_shapes:\n",
        "  print('처리할 이미지 크기: ', shape)\n",
        "  img = resize_img(img, shape)\n",
        "  img = gradient_ascent(img, iterations = iterations, stop = step, max_loss = max_loss)\n",
        "\n",
        "  upscaled_shrunk_original_img = resize_img(origianl_img, shape)\n",
        "\n",
        "  same_size_origianl = resize_img(original_img, shape)\n",
        "\n",
        "  lost_detail = same_size_original - upscaled_shrunk_original_img\n",
        "\n",
        "  img += lost_detail\n",
        "  shrunk_origianl_img = resize_img(original_img, shape)\n",
        "  save_img(img, fname='dream_at_scale_' + str(shape) + '.png')\n",
        "\n",
        "save_img(img, fanme='./satasets/final_dream.png')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}